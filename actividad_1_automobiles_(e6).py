# -*- coding: utf-8 -*-
"""Actividad 1 Automobiles (E6)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ulofsf-rYZZlV-ECky2kTAnWsnTh2N3n

# Importe y subida de archivos
"""

from google.colab import files
from IPython.display import Image
from google.colab import drive
import pandas as pd

"""1. **¿Qué es Supervised Machine Learning y cuáles son algunas de sus aplicaciones en Inteligencia de Negocios?**

El aprendizaje automático supervisado es una técnica de machine learning que emplea **datos etiquetados** para entrenar modelos capaces de realizar predicciones precisas. Estos modelos aprenden a asociar entradas específicas con salidas deseadas, lo que les permite generalizar a nuevos datos y tomar decisiones informadas.



2. **¿Cuáles son los principales algoritmos de Supervised Machine Learning? Brevemente describir con tus propias palarbas 5 – 7 de los principales algoritmos de Supervised Machine Learning.**


*  **Árboles de Decisión:** Utilizan una estructura similar a un árbol para ayudar a la toma de decisiones. Comienza en un nodo raíz y parten de ahí sus ramas con sus respectivas hojas, con cada rama representando una posible decisión basada en el valor de una característica. Al finalizar cada rama se encuentra su hoja, que representa la clase o categoría a la que pertenece cada dato. Este puede ser un modelo tanto de clasificación como de regresión, pero es más comúnmente utilizado para la clasificación.
*   **Redes Neuronales:** Las redes neuronales son un modelo principalmente inspirado en el cerebro humano, con el objetivo de replicar su manera de pensar y tomar decisiones. Junto con una validación cruzada efectiva, funcionan aprendiendo de los datos para identificar patrones y hacer predicciones. Son usualmente utilizadas para el reconocimiento de imágenes y el procesamiento de lenguaje natural.
*   **Regresión:** La regresión es una técnica estadística que busca la relación entre una variable de salida (dependiente) y una o más variables de entrada (independientes). Con un modelo de regresión, es posible estimar (y predecir) el valor de la variable de salida con base en los valores de las variables de entrada que se iincluyan. Por ejemplo, podemos utilizar la regresión para saber si una persona tiene una enfermedad cardíaca con base en sus niveles de colesterol, frecuencia cardíaca, edad, peso, y más factores.
*   **Árboles Aleatorios**: Los bosques aleatorios son un conjunto de árboles de decisión que, al combinarse, reducen el riesgo de sobreajuste. Esta técnica mejora la precisión de las predicciones al considerar múltiples perspectivas en la toma de decisiones.
*   **Máquinas de Vectores de Soporte (SVM):** Este modelo utiliza hiperplanos para separar datos de diferentes categorías. El objetivo es encontrar el hiperplano que maximiza la distancia entre los puntos de datos más cercanos de cada clase, conocido como margen. Las SVM son especialmente útiles para clasificar datos con muchas características, como imágenes y texto.


**3. ¿Qué es la R² Ajustada? ¿Qué es la métrica RMSE? ¿Cuál es la diferencia entre la R² Ajustada y la métrica RMSE?**

La R² se enfoca en la proporción de varianza explicada y penaliza la complejidad del modelo, y es generalmente utilizada para comparar modelos con diferentes números de predictores. Por otro lado, el RMSE se enfoca en el error promedio de las predicciones. Es útil para evaluar la precisión del modelo en términos de las unidades de la variable dependiente, y se interpreta de manera que los valores más bajos indican un mejor ajuste del modelo, ya que el error promedio es menor.

# Limpieza de datos
"""

df = pd.read_csv('automoble_insurance_claims.csv', encoding='ISO-8859-1')

"""Eliminar la columna *_c39*"""

df = df.drop(['_c39'], axis=1)

"""Reemplazar todas las ? de la columna property_damage a YES"""

df['property_damage'] = df['property_damage'].replace('?', 'YES')
df['property_damage'] = df['property_damage'].replace('YES',1)
df['property_damage'] = df['property_damage'].replace('NO',0)

# Yes = 1
# No = 0

"""Agrupar los niveles de educación a 3 niveles"""

def agrupa_educacion(nivel):
  if nivel in ['JD', 'MD', 'PhD', 'Masters']:
    return 'Alto'
  elif nivel in ['Associate', 'College']:
    return 'Medio'
  else:
    return 'Bajo'

df['insured_education_level'] = df['insured_education_level'].apply(agrupa_educacion)
df['insured_education_level'] = df['insured_education_level'].replace('Alto', 1)
df['insured_education_level'] = df['insured_education_level'].replace('Medio', 2)
df['insured_education_level'] = df['insured_education_level'].replace('Bajo', 3)

# Alto = 1
# Medio = 2
# Bajo = 3

"""Agrupar los dias de la semana a Weekend and Weekday"""

def agrupa_dias(fecha):
  dia_semana = pd.to_datetime(fecha).day_name()
  if dia_semana in ['Saturday', 'Sunday']:
    return 'Weekend'
  else:
    return 'Weekday'

df['policy_bind_date'] = df['policy_bind_date'].apply(agrupa_dias)
df['incident_date'] = df['incident_date'].apply(agrupa_dias)
df['policy_bind_date'] = df['policy_bind_date'].replace('Weekend', 1)
df['policy_bind_date'] = df['policy_bind_date'].replace('Weekday', 0)
df['incident_date'] = df['incident_date'].replace('Weekend', 1)
df['incident_date'] = df['incident_date'].replace('Weekday', 0)

#Weekend = 1
# Weekday = 0

"""Agrupar las horas del incidente a la clasificación mañana, tarde y noche"""

def agrupa_horas(hora):
  if hora >= 5 and hora < 12:
    return 'mañana'
  elif hora >= 12 and hora < 19:
    return 'tarde'
  else:
    return 'noche'

df['incident_hour_of_the_day'] = df['incident_hour_of_the_day'].apply(agrupa_horas)
df['incident_hour_of_the_day'] = df['incident_hour_of_the_day'].replace('mañana', 1)
df['incident_hour_of_the_day'] = df['incident_hour_of_the_day'].replace('tarde', 2)
df['incident_hour_of_the_day'] = df['incident_hour_of_the_day'].replace('noche', 3)

# mañana = 1
# tarde = 2
# noche = 3

"""Reemplazar en las columnas police_report_available y property_damage los valores ? a NO"""

df['police_report_available'] = df['police_report_available'].apply(lambda x: 'NO' if x != 'YES' else x)
df['property_damage'] = df['property_damage'].apply(lambda x: 'NO' if x != 'YES' else x)

df['police_report_available'] = df['police_report_available'].replace('YES', 1)
df['police_report_available'] = df['police_report_available'].replace('NO', 0)
df['property_damage'] = df['property_damage'].replace('YES', 1)
df['property_damage'] = df['property_damage'].replace('NO', 0)

# YES = 1
# NO = 0

"""Reemplazar en la columnas collusion_type los valores ? a Not detected"""

def reemplaza_collision(valor):
  if valor not in ['NO', 'YES']:
    return 'Not detected'
  else:
    return valor

df['collision_type'] = df['collision_type'].apply(reemplaza_collision)
df['collision_type'] = df['collision_type'].replace('Not detected', 0)
df['collision_type'] = df['collision_type'].replace('Rear Collision', 1)

"""Remplazar en la columna"""

def reemplaza_fraude(valor):
  if valor == 'Y':
    return 'YES'
  elif valor == 'N':
    return 'NO'
  else:
    return valor

df['fraud_reported'] = df['fraud_reported'].apply(reemplaza_fraude)
df['fraud_reported'] = df['fraud_reported'].replace('YES', 1)
df['fraud_reported'] = df['fraud_reported'].replace('NO', 0)

# YES = 1
# NO = 0

"""Cambiamos todos los valores de YES or NO a sistema binario"""

for columna in df.columns:
  df[columna] = df[columna].apply(lambda x: 1 if x == 'YES' else (0 if x == 'NO' else x))

"""Clasificar todos los diferentes tipos de autos"""

def clasifica_autos(modelo):
  if modelo in ['3 Series', 'E-Class', 'A5', 'A3', 'C-Class', 'X5', 'M5', '92x', '93', '95']:
    return 'Sedán'
  elif modelo in ['F150', 'Silverado', 'RAM', 'Tacoma', 'Wrangler', 'Escape', 'CRV', 'MDX', 'Highlander', 'Pathfinder']:
    return 'SUV'
  elif modelo in ['Civic', 'Corolla', 'Focus', 'Impreza', 'Elantra', 'Cruze', 'Jetta', 'Camry', 'Accord', 'Altima']:
    return 'Compacto'
  elif modelo in ['Mustang', 'Camaro', 'Challenger', 'Charger', 'Corvette', '911', 'Boxster', 'Cayman', 'TT', 'Z4']:
    return 'Deportivo'
  elif modelo in ['Malibu', 'Fusion', 'Legacy', 'Sonata', 'Optima', 'Passat', 'Mazda3', 'Mazda6']:
    return 'Familiar'
  elif modelo in ['Sierra', 'Tundra', 'Titan', 'F-250', 'Silverado 2500', 'RAM 2500']:
    return 'Pick-up'
  else:
    return 'Otros'

df['auto_model'] = df['auto_model'].apply(clasifica_autos)

"""Clasificamos los autos en 3 categorías:
*   Antiguo
*   Reciente
*   Moderno



"""

# Clasificamos los autos en 3 categorías: Antiguo, Reciente, Moderno utilizando auto_model y auto_year
def clasifica_edad_auto(fila):
  modelo = fila['auto_model']
  año = fila['auto_year']

  if año < 2010:
    return 'Antiguo'
  elif año >= 2010 and año < 2015:
    return 'Reciente'
  else:
    return 'Moderno'

df['edad_auto'] = df.apply(clasifica_edad_auto, axis=1)

df['edad_auto'] = df['edad_auto'].replace('Antiguo', 1)
df['edad_auto'] = df['edad_auto'].replace('Reciente', 2)
df['edad_auto'] = df['edad_auto'].replace('Moderno', 3)

# ANTIGUO = 1
# RECIENTE = 2
# MODERNO = 3

#  Corregimos los caracteres especiales con objeto de mejor legibilidad

df['auto_model'] = df['auto_model'].replace('SedÃ¡n', 'Sedán')

"""Exporte de base de datos"""

df.to_csv('data2.csv', index=False)

"""Dividr los data frame en objeto y número"""

df_texto = df.select_dtypes(include=['object'])
df_numerico = df.select_dtypes(include=['number'])

"""# Análisis Exploratorio
El presente análisis exploratorio tiene como objetivo fundamental sentar las bases para el desarrollo de modelos de machine learning predictivos en el ámbito de los seguros de automóviles. A través de un estudio detallado de la base de datos, se busca identificar patrones, tendencias y relaciones ocultas entre las variables, lo que permitirá comprender mejor la dinámica de los siniestros y los factores que influyen en el costo de las reparaciones.

# Medidas descriptivas

Las medidas descriptivas son herramientas estadísticas que nos permiten resumir y entender grandes conjuntos de datos. Actúan como un tipo de resumen que ofrece una visión general clara y concisa sobre la información que se está analizando.

A primera instancia y con base en los datos obtenidos, se puede concluir que el promedio de las personas llamando o solicitando un seguro de autómovil tienen 39 años, llevan alrededor de 17 años de ser clientes del seguro.
"""

df_limpio = pd.read_csv('data2.csv')

df_texto = df_limpio.select_dtypes(include=['object'])
df_numerico = df_limpio.select_dtypes(include=['number'])

descriptivas_numericas = df_numerico.describe(include='all')

tabla_descriptivas = pd.DataFrame({
    'Variable': descriptivas_numericas.columns,
    'Media': descriptivas_numericas.loc['mean'],
    'Mediana': descriptivas_numericas.loc['50%'],
    'Moda': df_numerico.mode().iloc[0]
})

print(tabla_descriptivas)

import matplotlib.pyplot as plt
import seaborn as sns
# Análisis de correlación
plt.figure(figsize=(15, 10))

# Using library Seaborn to create a correlation heatmap
sns.heatmap(df_numerico.corr(), annot=True, fmt='.2f', cmap='Pastel2', linewidths=2)

plt.title('Correlation Heatmap')
plt.show()

"""# Medidas de dispersión

Se observa una gran variabilidad en aspectos como la antigüedad de los clientes, el monto de las primas y los costos de los siniestros. Las variables relacionadas con el vehículo, como el año y el número de vehículos involucrados, muestran una menor dispersión. Además, se incluyen variables categóricas como la presencia de daños a la propiedad o lesiones corporales. Estos datos servirán como base para análisis más profundos y la construcción de modelos predictivos.
"""

rango_num = df_numerico.max() - df_numerico.min()
varianza_num = df_numerico.var()
desviacion_estandar_num = df_numerico.std()

medidas_dispersion = pd.DataFrame({
    'Rango': rango_num,
    'Varianza': varianza_num,
    'Desviación Estándar': desviacion_estandar_num
})
display(medidas_dispersion)

"""# Boxplots"""

import seaborn as sns
import matplotlib.pyplot as plt

# Seleccionar las columnas numéricas relevantes
columnas_numericas = ['vehicle_claim', 'property_claim', 'injury_claim', 'total_claim_amount', 'months_as_customer']

fig, axes = plt.subplots(nrows=len(columnas_numericas), ncols=1, figsize=(10, 20))

for i, columna in enumerate(columnas_numericas):
  sns.boxplot(x='auto_model', y=columna, data=df, ax=axes[i])
  axes[i].set_title(f'Boxplot de {columna} por Modelo de Auto')
  axes[i].set_xlabel('Modelo de Auto')
  axes[i].set_ylabel(columna)

plt.tight_layout()
plt.show()

"""**Hallazgos Boxplot**

El análisis de los datos de reclamos revela que los vehículos compactos presentan la menor frecuencia de siniestros y, cuando ocurren, los costos de reparación suelen ser inferiores al promedio. Por otro lado, los SUV destacan por una mayor incidencia de reclamos por daños a terceros, con montos que superan en un 20% el promedio general. En cuanto al costo total de los siniestros, tanto los SUV como otros modelos de mayor tamaño suelen generar reclamos superiores a los 60,000 pesos, mientras que los sedanes y familiares se ubican ligeramente por debajo de este umbral

# Histogramas
"""

# Seleccionar las columnas numéricas relevantes
columnas_numericas = ['vehicle_claim', 'property_claim', 'injury_claim', 'total_claim_amount', 'months_as_customer']

fig, axes = plt.subplots(nrows=len(columnas_numericas), ncols=1, figsize=(10, 20))

for i, columna in enumerate(columnas_numericas):
  sns.histplot(x=columna, data=df, hue='auto_model', ax=axes[i], multiple='stack')
  axes[i].set_title(f'Histograma de {columna} por Modelo de Auto')
  axes[i].set_xlabel(columna)
  axes[i].set_ylabel('Frecuencia')

plt.tight_layout()
plt.show()

"""**Hallazgos Histograma**

Los datos revelan que los vehículos SUV presentan la mayor frecuencia de reclamos por daños al vehículo, concentrándose en un rango de entre 40,000 y 50,000 pesos.

Asimismo, se ha identificado una tendencia en los reclamos según la antigüedad de la póliza. Los asegurados con una antigüedad de entre 220 y 280 meses presentan la mayor incidencia de siniestros, seguido de un grupo menor con una antigüedad de entre 100 y 150 meses. Por el contrario, los clientes con pólizas superiores a 300 meses muestran una menor probabilidad de sufrir accidentes.

Los modelos analizados indican una correlación entre los reclamos por daños a propiedad y los reclamos por lesiones, con un monto promedio por este último tipo de reclamo inferior a 10,000 pesos.

### Especificacion del Modelo e Hipotesis

### Modelo de Regresión y Variable Dependiente Seleccionada
Con base en los hallazgos del análisis exploratorio, el Precio Total del Reclamo (total_claim_amount) ha sido seleccionado como la variable objetivo de este proyecto. A través del desarrollo de un modelo predictivo robusto, se busca optimizar el proceso de valoración de los reclamos al seguro, ofreciendo a los clientes estimaciones más precisas y oportunas.

# Modelos de Supervised Machine Learning
A continuación se presentan los modelos de Machine Learning para su evaluación y aceptación para predecir la variable dependiente.
"""

# Carga de paquetería
import warnings
warnings.filterwarnings('ignore')
import numpy as np
import pandas as pd
from pandas import Series, DataFrame
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler, PolynomialFeatures
from sklearn.linear_model import LinearRegression, Lasso, Ridge
from sklearn.model_selection import train_test_split
from sklearn.neighbors import LocalOutlierFactor
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.model_selection import RandomizedSearchCV, cross_val_score, RepeatedStratifiedKFold
import sklearn
from sklearn.model_selection import train_test_split
from statsmodels.compat import lzip
import statsmodels.api as sm
import statsmodels.formula.api as smf
from statsmodels.formula.api import ols

"""**Especificación de variables**"""

# Abrimos el archivo limpio delimitado por comas
df_limpio = pd.read_csv('data2.csv')

#df_limpio = pd.DataFrame([df], index=[0])
df_limpio.info()

# Análisis de correlación
plt.figure(figsize=(15, 10))

# Using library Seaborn to create a correlation heatmap
sns.heatmap(df_numerico.corr(), annot=True, fmt='.2f', cmap='Pastel2', linewidths=2)

plt.title('Correlation Heatmap')
plt.show()

# Regresión entre una variable independiente y nuestra variable dependiente
sns.jointplot(data=df_numerico, x='vehicle_claim', y='injury_claim')

# Relación entre una variable independiente y nuestra variable dependiente
sns.jointplot(data=df_numerico, x='total_claim_amount', y='property_claim')

X = df_limpio[['injury_claim','police_report_available','umbrella_limit','edad_auto','number_of_vehicles_involved','incident_date']]
Y = df_limpio[['total_claim_amount']]

"""Realizamos un análisis para definir las variables que más nos convienen para nuestros modelos siguientes."""

import pandas as pd
import statsmodels.api as sm
from sklearn.preprocessing import StandardScaler

# Suponiendo que el DataFrame df_numerico ya está cargado
# df_numerico = pd.read_csv('path_to_your_data.csv')  # Asegúrate de cargar tus datos

# Variables dependientes
dependent_vars = ['total_claim_amount','vehicle_claim', 'property_claim', 'injury_claim']
# Variables explicativas: todas las columnas menos las dependientes
#explanatory_vars = [col for col in df_numerico.columns if col not in dependent_vars and col != 'total_claim_amount']
explanatory_vars = [col for col in df_numerico.columns if col not in dependent_vars]

# Preparar las variables explicativas y dependientes
X = df_numerico[explanatory_vars]
y = df_numerico[dependent_vars]

# Escalar las variables explicativas
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Convertir X_scaled a un DataFrame para usar en statsmodels
X_scaled_df = pd.DataFrame(X_scaled, columns=explanatory_vars)

# Función para la selección paso a paso
def stepwise_selection(X, y, threshold_in=0.05, threshold_out=0.10):
    """Perform stepwise regression."""
    initial_features = X.columns.tolist()
    best_features = []
    while initial_features:
        features_with_candidates = list(set(initial_features))
        features_with_candidates.sort()
        p_values = []
        for feature in features_with_candidates:
            model = sm.OLS(y, sm.add_constant(X[best_features + [feature]])).fit()
            p_values.append((feature, model.pvalues[feature]))
        p_values.sort(key=lambda x: x[1])

        if p_values and p_values[0][1] < threshold_in:
            best_features.append(p_values[0][0])
            initial_features.remove(p_values[0][0])
        else:
            break

        while best_features:
            model = sm.OLS(y, sm.add_constant(X[best_features])).fit()
            p_values = model.pvalues[1:]  # Excluir la constante
            if (p_values > threshold_out).any():
                worst_feature = p_values.idxmax()
                best_features.remove(worst_feature)
            else:
                break

    return best_features

# Realizar selección paso a paso para cada variable dependiente
for dep_var in dependent_vars:
    print(f"\nStepwise selection for {dep_var}:")
    best_features = stepwise_selection(X_scaled_df, y[dep_var])
    print(f"Best features for {dep_var}: {best_features}")

    # Ajustar el modelo final
    model = sm.OLS(y[dep_var], sm.add_constant(X_scaled_df[best_features])).fit()
    print(f"\nFinal model for {dep_var}:")
    print(model.summary())

"""La variable que presenta un AIC menor es property_claim, con un R-squared the 0.078. Asimismo, las variables más explicativas serán number_of_vehicles_involved, fraud_reported, e incident_hour_of_the_day"""

X_property = df_limpio[['number_of_vehicles_involved', 'vehicle_claim','fraud_reported', 'incident_hour_of_the_day']]
Y_property = df_limpio[['property_claim']]

"""Añadimos la variable vehicle_claim debido a su alta correlación con base en el análisis desarrollado con anterioridad. Sin embargo, realizamos también la prueba de multicolinearidad para verificar si esta se puede añadir al modelo. Parece que no hay multicolinearidad, por lo que podemos continuar con el desarrollo de nuestro modelo."""

# Multicollinearity
from statsmodels.stats.outliers_influence import variance_inflation_factor
VIF = pd.DataFrame()
VIF['Explanatory Variables'] = X_property.columns
VIF['VIF'] = [variance_inflation_factor(X_property.values,i) for i  in range(X_property.shape[1])]
VIF['VIF'] = round(VIF['VIF'],2)
VIF = VIF.sort_values(by = "VIF", ascending = False)
VIF

"""No hay multicolinearidad entre las variables, por lo que podemos proceder a la estimación de Modelo de Regresión"""

reg_model_1 = sm.OLS(Y,X).fit()

reg_model_1a = smf.ols(formula = 'property_claim ~ incident_hour_of_the_day + number_of_vehicles_involved + fraud_reported + vehicle_claim', data=df_limpio).fit()
print(reg_model_1a.summary())

"""Los resultados son similares a los obtenidos con anterioridad."""

df_numerico.var()

# RMSE OLS
OLS_pred = reg_model_1a.predict(df_limpio)
OLS_rmse = mean_squared_error(y_true = Y_property, y_pred = OLS_pred, squared = False)
OLS_rmse

"""Tenemos un RMSE relativamente considerable, pero es necesario crear otros modelos para determinar si es positivo el resultado o no."""

# the dependent variable is right - skewed
plt.hist(df_limpio['property_claim'])
plt.title("Histograma - Reclamo por propiedad")
plt.ylabel("Frecuencia")
plt.show()

"""Con las variables seleccionadas, nuestra variable dependiente tiene una distribución normal."""

# Check Normality of Estimated Regression Residuals
# print(reg_model_1.summary())
Y_predicted = reg_model_1a.predict(X_property)
residuals_reg_model_1 = df_limpio.property_claim - Y_predicted

# Plot the histogram of Estimated Regression Residuals
fig = plt.figure()
sns.distplot((residuals_reg_model_1))
fig.suptitle('Histogram of Residuals', fontsize = 20)
plt.xlabel('Residuals', fontsize = 18)
plt.show()

# Check Normality of Estimated Regression Residuals
name = ['Jarque-Bera', 'Chi^2 two-tail prob.', 'Skew', 'Kurtosis']
test = sm.stats.jarque_bera(reg_model_1a.resid)
lzip(name, test)

"""Los resultados del test de Jarque-Bera, junto con la probabilidad asociada (p-value), indican que los residuos del modelo no son normalmente distribuidos. Si los residuos no son normales, esto puede ser un indicio de que el modelo puede no estar capturando completamente la relación entre las variables."""

# Crear una nueva columna con el logaritmo de 'property_claim'
df_limpio['log_property_claim'] = np.log1p(df_limpio['property_claim'])

# Ajustar el modelo de regresión logarítmica
reg_model_log = smf.ols(formula='log_property_claim ~ incident_hour_of_the_day + number_of_vehicles_involved + fraud_reported + vehicle_claim', data=df_limpio).fit()

# Imprimir el resumen del modelo
print(reg_model_log.summary())

# Check Normality of Estimated Regression Residuals
Y_predicted_log = reg_model_log.predict(X_property)
residuals_reg_model_log = df_limpio.log_property_claim - Y_predicted_log

# Plot the histogram of Estimated Regression Residuals
fig = plt.figure()
sns.distplot((residuals_reg_model_log))
fig.suptitle('Histogram of Residuals (Log Transformed)', fontsize = 20)
plt.xlabel('Residuals', fontsize = 18)
plt.show()

# Jarque-Bera test for normality
name = ['Jarque-Bera', 'Chi^2 two-tail prob.', 'Skew', 'Kurtosis']
test = sm.stats.jarque_bera(reg_model_log.resid)
lzip(name, test)

# Check variance of residuals
plt.scatter(Y_predicted, residuals_reg_model_1)
plt.axhline(y=0, color='black', linestyle='-', linewidth=3)
plt.title('Model 1: Residuals vs Fitted Values', fontsize=16, weight='bold')
plt.xlabel('Fitted Values', fontsize = 10)
plt.ylabel('Residuals', fontsize = 10)
plt.show()

"""La transformación logarítmica hizo que nuestros residuos aumentaran y la distribución estuviera aún más variada, por lo que no funcionó la técnica de la transformación logarítmica. Por ende, continuaremos con el desarrollo de los siguientes modelos y así definir cuál es el que se desempeña de mejor manera.

# Modelo RIDGE
"""

# Define las variables explicativas y la variable objetivo
X = df_limpio[['incident_hour_of_the_day','number_of_vehicles_involved','fraud_reported','vehicle_claim']]
y = df_limpio['property_claim']

# Divide los datos en conjuntos de entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Crea el modelo Ridge
ridge_model = Ridge()

# Define los parámetros para la búsqueda de hiperparámetros
param_grid = {'alpha': np.logspace(-4, 4, 100)}

# Crea el objeto RandomizedSearchCV
grid_search = RandomizedSearchCV(estimator=ridge_model, param_distributions=param_grid, n_iter=100, cv=5, scoring='neg_mean_squared_error', random_state=42)

# Ajusta el modelo a los datos de entrenamiento
grid_search.fit(X_train, y_train)

# Obtiene el mejor modelo
best_ridge_model = grid_search.best_estimator_

# Imprime los mejores parámetros
print("Mejores parámetros:", grid_search.best_params_)

# Evalúa el modelo en el conjunto de prueba
y_pred = best_ridge_model.predict(X_test)
rmse = mean_squared_error(y_test, y_pred, squared=False)
print("RMSE:", rmse)

# prompt: consigue la r squared del modelo anterior

r_squared = reg_model_1a.rsquared
print("R-squared:", r_squared)

print("RMSE:", rmse)

"""Hasta el momento y con un RMSE de 3,352.41, nuestro modelo de RIDGE es el que mejor explica los resultados. Tiene un R-squared de 0.54, muy parecido al modelo de regresión.

Como lo determinamos con anterioridad, nuestras mejores variables para el modelo RIDGE son fraud_reported, number_of_vehicles_involved, vehicle_claim, e incident_hour_of_the_day. La variable de vehicle_claim tiene un coeficiente un poco más cercano a cero, pero lo conservaremos por ahora para evaluar los siguientes modelos.
"""

# Define las variables explicativas y la variable objetivo
X = df_limpio[['fraud_reported','vehicle_claim','number_of_vehicles_involved', 'incident_hour_of_the_day']]
y = df_limpio['property_claim']

# Divide los datos en conjuntos de entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Crea el modelo Ridge
ridgereg = Ridge(alpha=0.075)

# Ajusta el modelo a los datos de entrenamiento
ridgereg.fit(X_train, y_train)

# Crea un DataFrame con los coeficientes
coeff_RIDGE = pd.DataFrame(X.columns, columns=['Features'])
coeff_RIDGE['Coefficient Estimate'] = Series(ridgereg.coef_)
print(coeff_RIDGE)

"""La mayoría de las variables tiene un impacto positivo en nuestra variable dependiente, siendo la variable con más impacto fraud_reported."""

predictors = X.columns
coef_ridge = Series(ridgereg.coef_, predictors).sort_values()
coef_ridge.plot(kind='bar', title='Ridge Estimated Coefficients')

"""Volvimos a correr nuestro diagnóstico de multicolinearidad para corroborar que ninguna de nuestras variables estuviera relacionada con otra."""

from statsmodels.stats.outliers_influence import variance_inflation_factor

X = df_limpio[['number_of_vehicles_involved','vehicle_claim','fraud_reported','incident_hour_of_the_day']]

vif_data = pd.DataFrame()
vif_data["feature"] = X.columns
vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]

print(vif_data)

"""# Modelo LASSO"""

# Define las variables explicativas y la variable objetivo
X = df_limpio[['fraud_reported', 'vehicle_claim','number_of_vehicles_involved', 'incident_hour_of_the_day']]
y = df_limpio['property_claim']

# Divide los datos en conjuntos de entrenamiento y prueba
X_train_lasso, X_test_lasso, y_train_lasso, y_test_lasso = train_test_split(X, y, test_size=0.2, random_state=42)

# Crea el modelo Lasso
lasso_model = Lasso()

# Define los parámetros para la búsqueda de hiperparámetros
param_grid = {'alpha': np.logspace(-4, 4, 100)}

# Crea el objeto RandomizedSearchCV
grid_search = RandomizedSearchCV(estimator=lasso_model, param_distributions=param_grid, n_iter=100, cv=5, scoring='neg_mean_squared_error', random_state=42)

# Ajusta el modelo a los datos de entrenamiento
grid_search.fit(X_train_lasso, y_train_lasso)

# Obtiene el mejor modelo
best_lasso_model = grid_search.best_estimator_

# Imprime los mejores parámetros
print("Mejores parámetros:", grid_search.best_params_)

# Evalúa el modelo en el conjunto de prueba
y_pred_lasso = best_lasso_model.predict(X_test_lasso)
rmse = mean_squared_error(y_test_lasso, y_pred_lasso, squared=False)
print("RMSE:", rmse)

r_squared = best_lasso_model.score(X_test_lasso, y_test_lasso)
print("R-squared:", r_squared)

print("RMSE:", rmse)

# Define las variables explicativas y la variable objetivo
X = df_limpio[['fraud_reported', 'vehicle_claim','number_of_vehicles_involved', 'incident_hour_of_the_day']]
y = df_limpio['property_claim']

# Divide los datos en conjuntos de entrenamiento y prueba
X_train_lasso, X_test_lasso, y_train_lasso, y_test_lasso = train_test_split(X, y, test_size=0.2, random_state=42)

# Crea el modelo Lasso
lassoreg = Lasso(alpha=0.05)  # Puedes ajustar el valor de alpha

# Ajusta el modelo a los datos de entrenamiento
lassoreg.fit(X_train_lasso, y_train_lasso)

# Crea un DataFrame con los coeficientes
coeff_LASSO = pd.DataFrame(X.columns, columns=['Features'])
coeff_LASSO['Coefficient Estimate'] = Series(lassoreg.coef_)
print(coeff_LASSO)

# La mayoría de las variables tiene un impacto positivo en nuestra variable dependiente, siendo la variable con más impacto number_of_vehicles_involved. Mientras tanto, la variable edad_auto tiene impacto negativo medianamente fuerte en nuestra variable predictora.
predictors = X.columns
coef_lasso = Series(lassoreg.coef_, predictors).sort_values()
coef_lasso.plot(kind='bar', title='Lasso Estimated Coefficients')

"""Corremos el modelo de regresión RIDGE con un alpha ajustada. fraud_reported continúa siendo la variable que tiene un mayor impacto en nuestra variable dependiente, y logramos subir nuestra r-squared. Sin embargo, nuestra variable vehicle_claim es la que menos impacto tiene a nuestra variable dependiente."""

# RIDGE Regression
from sklearn.linear_model import Ridge
ridgereg = Ridge(alpha=0.075)
ridgereg.fit(X, Y)
RIDGE_pred = ridgereg.predict(X)
ridgereg.score(X,Y) # R-Squared

"""# Polynomial Regression"""

from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import pandas as pd

# Define las variables explicativas y la variable objetivo
X = df_limpio[['fraud_reported','vehicle_claim', 'number_of_vehicles_involved', 'incident_hour_of_the_day']]
y = df_limpio['property_claim']

# Divide los datos en conjuntos de entrenamiento y prueba
X_train_poly, X_test_poly, y_train_poly, y_test_poly = train_test_split(X, y, test_size=0.2, random_state=42)

# Crea el transformador polinomial
poly = PolynomialFeatures(degree=3)  # Puedes ajustar el grado del polinomio

# Transforma las variables explicativas
X_train_poly = poly.fit_transform(X_train)
X_test_poly = poly.transform(X_test)

# Crea el modelo de regresión lineal
poly_reg_model = LinearRegression()

# Ajusta el modelo a los datos de entrenamiento transformados
poly_reg_model.fit(X_train_poly, y_train)

# Predice en el conjunto de prueba
y_pred_poly = poly_reg_model.predict(X_test_poly)

# Evalúa el modelo con RMSE
rmse_poly = mean_squared_error(y_test, y_pred_poly, squared=False)
print("RMSE (Polynomial Regression):", rmse_poly)

r_squared_poly = poly_reg_model.score(X_test_poly, y_test)
print("R-squared (Polynomial Regression):", r_squared_poly)

polynomial_model = smf.ols(formula= 'property_claim ~ fraud_reported + number_of_vehicles_involved + incident_hour_of_the_day + vehicle_claim', data=df_limpio).fit()
print(polynomial_model.summary())

Polynomial_pred = polynomial_model.predict(df_limpio)
Polynomial_rmse = mean_squared_error(y_true = Y, y_pred= Polynomial_pred, squared = False)
Polynomial_rmse

"""Obtuvimos un r-squared y RMSE distintos debido a las librerías utilizadas y los datos ingresados en las funciones. Sin embargo, para objetos de este trabajo, se utilizarán los segundos datos, con el R-squared de 0.537 (el modelo explica un 53.7% de los datos) y un RMSE de 50,826.498. Hasta ahora, el model polinomial ha dado el RMSE más alto, por lo que no parece prometedor."""

# Define las variables explicativas y la variable objetivo
X = df_limpio[['fraud_reported', 'number_of_vehicles_involved', 'incident_hour_of_the_day','vehicle_claim']]

# Crea una nueva columna con la edad al cuadrado
#X['age_squared'] = X['age'] ** 2

# Calcula VIF
VIF = pd.DataFrame()
VIF['Explanatory Variables'] = X.columns
VIF['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
VIF['VIF'] = round(VIF['VIF'], 2)
VIF = VIF.sort_values(by="VIF", ascending=False)

# Display the VIF results
print(VIF)

# Plotting a graph for actual vs predicted values
predict_reg_model_1a = reg_model_1a.predict(df_limpio)
fig = plt.figure()
plt.scatter(Y, predict_reg_model_1a)
fig.suptitle('Y Actual vs Predicción | R2 = 0.53', fontsize = 14)
plt.xlabel('Y Actual', fontsize = 18)
plt.ylabel('Predicción', fontsize = 16)
plt.show()

"""El modelo actual tiene un rendimiento moderado (R² = 0.53), lo que indica que es capaz de predecir correctamente el 53% de los datos."""

fig = sm.graphics.plot_partregress_grid(polynomial_model)
fig.tight_layout(pad=1.0)

import seaborn as sns
import matplotlib.pyplot as plt
sns.regplot(x = "property_claim", y = "vehicle_claim",  ci = None, data = df_limpio)

"""# XGBOOST REGRESSION"""

import xgboost as xgb

xgboost_model = xgb.XGBRegressor(objective='reg:linear')
xgboost_model = xgb.XGBRegressor(max_depth=4, n_estimators=500, objective='reg:linear')
xgboost_model.fit(X, Y)
xgboost_predictions = xgboost_model.predict(X)

X = df_limpio[['fraud_reported', 'number_of_vehicles_involved', 'incident_hour_of_the_day', 'vehicle_claim']]
Y = df_limpio[['property_claim']]

xgboost_model = xgb.XGBRegressor(objective='reg:linear')
xgboost_model = xgb.XGBRegressor(max_depth=4, n_estimators=500, objective='reg:linear')
xgboost_model.fit(X, Y)
xgboost_predictions = xgboost_model.predict(X)

xgboost_rmse = mean_squared_error(Y, xgboost_predictions, squared = False)
xgboost_rmse
# xgboost_r2 = r2_score(Y, xgboost_predictions)

feature_importance = xgboost_model.feature_importances_
feature_importance

xgboost_feature_importance = pd.DataFrame({'importance':xgboost_model.feature_importances_,'feature':X.columns})
xgboost_feature_importance.sort_values('importance', ascending=False)

from xgboost import XGBRegressor
from sklearn.model_selection import GridSearchCV # GridSearchCV is one of the most widely used techniques for hyperparameter tuning. It involves specifying a set of possible values for each hyperparameter, and then training and evaluating the model for each combination of hyperparameter values.

param_grid = {

    'learning_rate': [0.1, 0.15, 0.25],

    'max_depth': [4, 6, 8],

    'min_child_weight': [1, 3, 5],

    'subsample': [0.8, 0.9, 1.0]

}

grid_search = GridSearchCV(XGBRegressor(), param_grid, cv=3)
grid_search.fit(X,Y)
best_params = grid_search.best_params_

# Hyperparameter Tunining of XGBoost model
xgb_model = XGBRegressor(**best_params)
xgb_model.fit(X,Y)

"""Nuestra variable que tiene una relevancia directa con nuestra variable dependiente es vehicle_claim, seguida de number_of_vehicles_involved."""

xgboost_fi = pd.DataFrame({'importance':xgb_model.feature_importances_,'feature':X.columns})
xgboost_fi.sort_values('importance', ascending=False)

plt.figure(figsize=(10,8))
sns.barplot(x=xgboost_fi['importance'], y=xgboost_fi['feature'],order=xgboost_fi.sort_values('importance', ascending=False)['feature'])
plt.title('XGBoost Variables Importance')
plt.xlabel('Variables Importance')
plt.ylabel('Variables Name')

"""Hasta el momento, nuestro modelo de XGBoost ha sido el que mejor RMSE tiene."""

XGBoost_hyp_rmse = np.sqrt(mean_squared_error(Y,xgb_model.predict(X)))
XGBoost_hyp_rmse

"""#Modelos para detectar Multicolinealidad"""

import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor


# Selecting numeric features relevant for multicollinearity check
numeric_features = [
    'months_as_customer', 'age', 'policy_deductable',
    'policy_annual_premium', 'umbrella_limit',
    'capital-gains', 'capital-loss', 'incident_hour_of_the_day',
    'number_of_vehicles_involved', 'bodily_injuries',
    'total_claim_amount', 'injury_claim', 'property_claim',
    'vehicle_claim'
]

# Subset the dataframe with only numeric features
X = df[numeric_features]

# Add a constant (intercept term) to the model
X = sm.add_constant(X)

# Calculate VIF for each feature
vif_data = pd.DataFrame()
vif_data["feature"] = X.columns
vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]

# Display VIF data
print(vif_data)

"""# Evaluación y Selección de Modelo de Regresión"""

# import numpy as np
# model_names = ['OLS', 'Ridge', 'Lasso', 'Polynomial', 'XGBoost']
# rmse_values = [OLS_rmse, RIDGE_rmse, LASSO_rmse, Polynomial_rmse, XGBoost_hyp_rmse]
# plt.figure(figsize=(10, 6))
# plt.bar(model_names, rmse_values, color='skyblue')
# plt.xlabel('Modelos')
# plt.ylabel('RMSE')
# plt.title('Comp

import matplotlib.pyplot as plt
import numpy as np

# Assuming these values are already defined from your previous code
model_names = ['Polynomial','XGBoost', 'OLS', 'Ridge', 'Lasso' ]
rmse_values = [Polynomial_rmse, XGBoost_hyp_rmse, OLS_rmse, rmse, rmse,]

# Create the bar plot
plt.figure(figsize=(10, 6))
plt.bar(model_names, rmse_values, color='skyblue')
plt.xlabel('Modelos')
plt.ylabel('RMSE')
plt.title('Comparación de Modelos de Regresión')
plt.show()

# Create a table with model names and RMSE values
data = {'Model': model_names, 'RMSE': rmse_values}
df_results = pd.DataFrame(data)
print(df_results)

"""Finalmente, se presenta una breve descripción de los principales hallazgos del Análisis Exploratorio de Datos (EDA) basado en las estadísticas descriptivas de las variables:

1. Antigüedad de la póliza: Los asegurados con una antigüedad de entre 220 y 280 meses presentan una mayor frecuencia de siniestros, mientras que aquellos con más de 300 meses muestran una menor probabilidad de accidentes.

2. Edad del vehículo: Los vehículos SUV tienden a generar los reclamos de mayor costo, especialmente en el rango de 40,000 a 50,000 pesos, mientras que los sedanes y vehículos familiares suelen estar por debajo de los 60,000 pesos.

3. Incidencia de reclamos según el tipo de vehículo: Los SUV destacan por una mayor incidencia de reclamos por daños a terceros, con montos que superan en un 20% el promedio general, mientras que los vehículos compactos tienen la menor frecuencia y costos de reparación más bajos.

4. Correlación entre reclamos: Existe una correlación significativa entre los reclamos por daños a la propiedad y los reclamos por lesiones, con un monto promedio de los reclamos por lesiones inferior a 10,000 pesos.

5. Fraude en los reclamos: La variable "fraud_reported" muestra una correlación notable con la variable explicativa "total_claim_amount", lo que sugiere que los montos de reclamos son mayores en casos de fraude.

6. Desviaciones significativas en los datos: Variables como "policy_number" y "umbrella_limit" presentan una alta varianza, indicando una gran dispersión en los datos, mientras que variables como "collision_type" y "property_damage" tienen desviaciones estándar de cero, lo que indica falta de variabilidad.

7. Impacto de la educación del asegurado: El nivel educativo del asegurado muestra una distribución bastante uniforme, sin una tendencia clara, lo que podría influir en la evaluación de riesgos y primas.

8. Distribución horaria de los incidentes: La mayoría de los incidentes se concentran en ciertas horas del día, con un pico en las primeras horas, lo que podría ser relevante para la evaluación del riesgo temporal.

9. Reclamos por tipo de daño: Los reclamos por daños al vehículo representan la mayor parte del total de reclamos, con montos promedio que varían significativamente según el tipo de vehículo y el contexto del siniestro.

10. Tendencias en los reclamos: Se observan patrones claros en la frecuencia y el monto de los reclamos según la antigüedad del cliente y del vehículo, lo que puede ayudar a predecir futuros siniestros y ajustar las políticas de aseguramiento.

# Selección del Mejor Modelo

Basándonos en los valores de RMSE, el modelo XGBoost es claramente el mejor de los cinco. Su RMSE es significativamente menor que el de los otros modelos, lo que indica que, en promedio, las predicciones del modelo XGBoost están mucho más cerca de los valores reales.

De acuerdo con la gráfica, las variables que más contribuyen a explicar los cambios en el monto total del reclamo son:

1. vehicle_claim: La variable "Vehicle Claim" es, por mucho, la variable más importante. Esto sugiere que la gravedad de los accidentes de vehículo tiene un impacto directo y significativo en la cantidad de reclamos por daño a la propiedad. De esto se podría inferir que es relacionado a accidentes automovilísticos, donde terceros también se ven involucrados.
2. fraud_reported: El hecho de que se presente un reclamo por daño a la propiedad está parcialmente relacionado con que haya un fraude reportado. Sería recomendable determinar las causas de estos reclamos y por qué es que en estos casos es en donde más se presentan los fraudes.

En conclusión, todos los modelos logran explicar una porción considerable de la variabilidad en el monto total del reclamo. Esto indica que las variables incluidas en el modelo son relevantes y contribuyen a la predicción del valor objetivo. Sin embargo, aunque todos los modelos evaluados proporcionan resultados razonables, el modelo XGBoost se destaca por su mayor precisión y capacidad para capturar relaciones más complejas entre las variables. Esto lo convierte en la mejor opción para predecir el monto total del reclamo en este caso particular.
"""